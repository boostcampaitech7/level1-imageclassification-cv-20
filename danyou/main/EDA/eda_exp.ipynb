{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 학습 데이터의 경로와 정보를 가진 파일의 경로를 설정\n",
    "traindata_dir = \"./data/train\"\n",
    "traindata_info_file = \"./data/train.csv\"\n",
    "\n",
    "# 테스트 데이터의 경로와 정보를 가진 파일의 경로를 설정\n",
    "testdata_dir = \"./data/test\"\n",
    "testdata_info_file = \"./data/test.csv\"\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "# 학습 데이터의 class, image path, target에 대한 정보가 들어있는 csv파일을 읽기\n",
    "train_data = pd.read_csv(traindata_info_file)\n",
    "\n",
    "# 테스트 데이터\n",
    "test_data = pd.read_csv(testdata_info_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터를 벡터화 (이미지 크기를 64x64로 통일했다고 가정)\n",
    "def image_to_vector(img_path):\n",
    "    if not os.path.exists(traindata_dir+'/'+img_path):\n",
    "        print(f\"File not found: {traindata_dir+'/'+img_path}\")\n",
    "        return None\n",
    "    img = cv2.imread(traindata_dir+'/'+img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"Could not load image: {traindata_dir+'/'+img_path}\")\n",
    "        return None\n",
    "    img_resized = cv2.resize(img, (64, 64))  # 이미지 크기를 64x64로 조정\n",
    "    return img_resized.flatten()  # 1D 벡터로 변환\n",
    "\n",
    "# 모든 이미지 벡터화\n",
    "def extract_features(df):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for img_path, label in zip(df['image_path'], df['target']):\n",
    "        feature = image_to_vector(img_path)\n",
    "        if feature is not None:  # 이미지 로드가 성공했을 경우에만 추가\n",
    "            features.append(feature)\n",
    "            labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# features와 labels 준비\n",
    "features, labels = extract_features(train_data)\n",
    "\n",
    "# features를 표준화\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# 클래스 별로 나누어 시각화\n",
    "def visualize_with_tsne(features, labels, classes, perplexity=30, learning_rate=200, n_iter=1000):\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter)\n",
    "    reduced = tsne.fit_transform(features)\n",
    "    \n",
    "    for i in range(0, len(classes), 10):  # 50개씩 나누기\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        subset_classes = classes[i:i+10]\n",
    "        mask = np.isin(labels, subset_classes)\n",
    "        reduced_subset = reduced[mask]\n",
    "        labels_subset = labels[mask]\n",
    "        sns.scatterplot(x=reduced_subset[:, 0], y=reduced_subset[:, 1], hue=labels_subset, palette='tab10', s=10)\n",
    "        plt.title(f't-SNE Visualization for Classes {subset_classes[0]} to {subset_classes[-1]}')\n",
    "        plt.show()\n",
    "\n",
    "# 클래스 목록 생성\n",
    "unique_classes = np.unique(labels)\n",
    "\n",
    "# 시각화 실행\n",
    "visualize_with_tsne(scaled_features, labels, unique_classes, perplexity=40, learning_rate=300, n_iter=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_to_vector(img_path, transform=None):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img_resized = cv2.resize(img, (64, 64))\n",
    "    img_normalized = img_resized.astype(np.uint8)  # uint8 타입으로 유지\n",
    "\n",
    "    if transform is not None:\n",
    "        img_normalized = transform(image=img_normalized)['image']\n",
    "    \n",
    "    return img_normalized.flatten() / 255.0  # 변환 후에 정규화\n",
    "\n",
    "# 모든 이미지 벡터화\n",
    "def extract_features_from_csv(csv_file, is_train=False):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # 공통 변환 설정\n",
    "    common_transforms = [\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    "\n",
    "    # 훈련용 변환 설정\n",
    "    transform = None\n",
    "    if is_train:\n",
    "        transform = A.Compose([\n",
    "            A.Rotate(limit=10, p=0.5),\n",
    "            A.Affine(scale=(0.8, 1.2), shear=(-10, 10), p=0.5),\n",
    "            A.ElasticTransform(alpha=1, sigma=10, alpha_affine=None, p=0.5),\n",
    "            # A.Erosion(kernel=(1, 2), p=0.5),\n",
    "            # A.Dilation(kernel=(1, 2), p=0.5),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "            A.MotionBlur(blur_limit=(3, 7), p=0.5),\n",
    "            A.CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=255, p=0.5),\n",
    "            A.OneOf([\n",
    "                # A.AutoContrast(p=0.5),\n",
    "                A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5),\n",
    "            ], p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5)\n",
    "        ] + common_transforms)\n",
    "\n",
    "    for img_path, label in zip(df['image_path'], df['target']):\n",
    "        img_path_full = os.path.join('./data/train/', img_path)  # 경로 수정\n",
    "        feature = image_to_vector(img_path_full, transform)\n",
    "        if feature is not None:\n",
    "            features.append(feature)\n",
    "            labels.append(label)  # 클래스 레이블 추가\n",
    "            \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# 데이터 전처리\n",
    "def preprocess_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "# Autoencoder 모델 정의\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# CSV 파일 경로 설정\n",
    "csv_file = './data/train.csv'\n",
    "# 특징 추출 (훈련 데이터)\n",
    "features, labels = extract_features_from_csv(csv_file, is_train=True)\n",
    "# print(f\"Number of features: {len(features)}\")\n",
    "# print(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# 데이터 전처리\n",
    "processed_data = preprocess_data(features)\n",
    "tensor_data = torch.FloatTensor(processed_data).cuda()  # CUDA로 이동\n",
    "\n",
    "# Autoencoder 훈련\n",
    "input_dim = tensor_data.shape[1]\n",
    "model = Autoencoder(input_dim).cuda()  # CUDA로 이동\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(tensor_data)\n",
    "    loss = criterion(output, tensor_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 원본 데이터와 재구성된 데이터 비교 함수 (클래스별)\n",
    "def visualize_comparison_by_class(original_data, reconstructed_data, labels, num_images=5):\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        idx = np.where(labels == label)[0]  # 클래스에 해당하는 인덱스\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i in range(min(num_images, len(idx))):\n",
    "            # 원본 이미지\n",
    "            ax = plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(original_data[idx[i]].detach().cpu().numpy().reshape(224, 224), cmap='gray')\n",
    "            plt.title(f\"Original: {label}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "            # 재구성된 이미지\n",
    "            ax = plt.subplot(2, num_images, i + 1 + num_images)\n",
    "            plt.imshow(reconstructed_data[idx[i]].detach().cpu().numpy().reshape(224, 224), cmap='gray')\n",
    "            plt.title(f\"Reconstructed: {label}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# 원본 데이터와 재구성된 데이터 비교\n",
    "visualize_comparison_by_class(tensor_data, model(tensor_data).cpu(), labels)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
